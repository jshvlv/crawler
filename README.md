# Async Crawler

–ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤–µ–±-–∫—Ä–∞—É–ª–µ—Ä –Ω–∞ Python —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π rate limiting, robots.txt, –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.

## –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- ‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –æ–±—Ö–æ–¥ —Å–∞–π—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
- üö¶ Rate limiting (–≥–ª–æ–±–∞–ª—å–Ω–æ –∏ –ø–æ –¥–æ–º–µ–Ω–∞–º)
- ü§ñ –°–æ–±–ª—é–¥–µ–Ω–∏–µ robots.txt
- üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–≤—Ç–æ—Ä—ã —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º backoff
- üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ HTML –æ—Ç—á—ë—Ç—ã
- üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON, CSV –∏ SQLite
- üó∫Ô∏è –ü–æ–¥–¥–µ—Ä–∂–∫–∞ sitemap.xml
- ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ YAML —Ñ–∞–π–ª
- üìù –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- üñ•Ô∏è CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞

```bash
pip install -r requirements.txt
```

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CLI

```bash
# –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—É—Å–∫
PYTHONPATH=./src python -m crawler.cli --urls https://example.com --max-pages 50

# –° –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–º —Ñ–∞–π–ª–æ–º
PYTHONPATH=./src python -m crawler.cli --config config.yaml

# –° –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
PYTHONPATH=./src python -m crawler.cli \
  --urls https://example.com \
  --max-pages 100 \
  --max-depth 2 \
  --rate-limit 2.0 \
  --respect-robots \
  --output results.json
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–¥–µ

```python
import asyncio
from crawler import AdvancedCrawler

async def main():
    # –ò–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    crawler = AdvancedCrawler.from_config("config.yaml")
    
    # –ò–ª–∏ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –≤—Ä—É—á–Ω—É—é
    from crawler.config import ConfigLoader
    config = ConfigLoader.DEFAULT_CONFIG.copy()
    config["urls"]["start_urls"] = ["https://example.com"]
    crawler = AdvancedCrawler(config=config)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∫—Ä–∞—É–ª–∏–Ω–≥
    results = await crawler.crawl()
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = crawler.get_stats()
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['total_pages']} —Å—Ç—Ä–∞–Ω–∏—Ü")
    print(f"–£—Å–ø–µ—à–Ω–æ: {stats['successful']}")
    print(f"–û—à–∏–±–æ–∫: {stats['failed']}")
    
    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç—ã
    crawler.export_to_json("stats.json")
    crawler.export_to_html_report("report.html")
    
    await crawler.close()

asyncio.run(main())
```

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `config.yaml` –Ω–∞ –æ—Å–Ω–æ–≤–µ `config.example.yaml`:

```yaml
crawler:
  max_concurrent: 5
  max_depth: 2
  max_pages: 50
  requests_per_second: 2.0
  respect_robots: true

urls:
  start_urls:
    - "https://example.com"
  use_sitemap: true

storage:
  type: "json"
  json:
    filename: "results.json"
```

## –ü–∞—Ä–∞–º–µ—Ç—Ä—ã CLI

- `--urls` - –°—Ç–∞—Ä—Ç–æ–≤—ã–µ URL (–º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ)
- `--max-pages` - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
- `--max-depth` - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –æ–±—Ö–æ–¥–∞
- `--output` - –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- `--config` - –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
- `--respect-robots` - –°–æ–±–ª—é–¥–∞—Ç—å robots.txt
- `--rate-limit` - –õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
- `--max-concurrent` - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
- `--same-domain-only` - –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ URL —Ç–æ–≥–æ –∂–µ –¥–æ–º–µ–Ω–∞
- `--use-sitemap` - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å sitemap.xml
- `--log-level` - –£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (DEBUG, INFO, WARNING, ERROR)
- `--log-file` - –§–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å–∏ –ª–æ–≥–æ–≤
- `--stats-json` - –§–∞–π–ª –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ JSON
- `--stats-html` - –§–∞–π–ª –¥–ª—è HTML –æ—Ç—á—ë—Ç–∞

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
src/crawler/
‚îú‚îÄ‚îÄ advanced_crawler.py  # –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å AdvancedCrawler
‚îú‚îÄ‚îÄ fetcher.py           # –û—Å–Ω–æ–≤–Ω–æ–π –∫—Ä–∞—É–ª–µ—Ä AsyncCrawler
‚îú‚îÄ‚îÄ parser.py            # –ü–∞—Ä—Å–µ—Ä HTML
‚îú‚îÄ‚îÄ queue_manager.py     # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—á–µ—Ä–µ–¥—å—é –∏ —Å–µ–º–∞—Ñ–æ—Ä–∞–º–∏
‚îú‚îÄ‚îÄ rate_limiter.py      # Rate limiting –∏ robots.txt
‚îú‚îÄ‚îÄ retry.py             # –ü–æ–≤—Ç–æ—Ä—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
‚îú‚îÄ‚îÄ sitemap.py           # –ü–∞—Ä—Å–µ—Ä sitemap.xml
‚îú‚îÄ‚îÄ stats.py             # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –æ—Ç—á—ë—Ç—ã
‚îú‚îÄ‚îÄ storage.py           # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
‚îú‚îÄ‚îÄ config.py            # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
‚îî‚îÄ‚îÄ cli.py               # CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
```

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –¢–µ—Å—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–Ω—è
PYTHONPATH=./src python tests/day1-tests.py
PYTHONPATH=./src python tests/day2-tests.py
PYTHONPATH=./src python tests/day3-tests.py
PYTHONPATH=./src python tests/day4-tests.py
PYTHONPATH=./src python tests/day5-tests.py
PYTHONPATH=./src python tests/day6-tests.py
```

## –õ–∏—Ü–µ–Ω–∑–∏—è

MIT

